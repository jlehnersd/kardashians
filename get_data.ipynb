{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import copy\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gensim\n",
    "# from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "# # sklearn\n",
    "# from sklearn import datasets\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# # logging for gensim (set to INFO)\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set notebook options ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 15)\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape data if not already downloaded ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kim is a basic bitch (already downloaded)\n",
      "khloe is a basic bitch (already downloaded)\n",
      "kourtney is a basic bitch (already downloaded)\n",
      "kris is a basic bitch (already downloaded)\n",
      "kylie is a basic bitch (already downloaded)\n",
      "kendall is a basic bitch (already downloaded)\n",
      "cailtyn is a basic bitch (already downloaded)\n",
      "rob is a basic bitch (already downloaded)\n"
     ]
    }
   ],
   "source": [
    "names = 'kim khloe kourtney kris kylie kendall cailtyn rob'.split()\n",
    "filenames = ['data/' + name + '.csv' for name in names]\n",
    "\n",
    "for idx, file in enumerate(filenames):\n",
    "    if not os.path.exists('data/kim.csv'):\n",
    "        print('Scraping Kim... (write scrape code)')\n",
    "    else:\n",
    "        name = names[idx]\n",
    "        print(f'{name} is a basic bitch (already downloaded)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in and combine data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caitlyn  = pd.read_csv('data/caitlyn.csv' , low_memory=False)\n",
    "kendall  = pd.read_csv('data/kendall.csv' , low_memory=False)\n",
    "kylie    = pd.read_csv('data/kylie.csv'   , low_memory=False)\n",
    "kim      = pd.read_csv('data/kim.csv'     , low_memory=False)\n",
    "kourtney = pd.read_csv('data/kourtney.csv', low_memory=False)\n",
    "khloe    = pd.read_csv('data/khloe.csv'   , low_memory=False)\n",
    "kris     = pd.read_csv('data/kris.csv'    , low_memory=False)\n",
    "#rob      = pd.read_csv('data/rob.csv'     , low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kardashians_ = pd.concat([caitlyn, kendall, kylie, kim, kourtney, khloe, kris, rob])\n",
    "kardashians_ = pd.concat([kendall, kylie, kim, kourtney, khloe, kris])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "kardashians = kardashians_[['date', 'time', 'username', 'tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Can’t leave the house without my must-have skincare products! #proactiv_ambassador  http://proactv.us/NgPyPZ  pic.twitter.com/si6kAzr6qg\n",
       "1                                   I’m so excited for you! ❤️#proactiv_ambassador https://twitter.com/mhabeck89/status/1143325557574295552 …\n",
       "2                                                    ❤️ #proactiv_ambassador https://twitter.com/HopkinsonKellie/status/1154176575006478336 …\n",
       "3                                         my @moonoralcare must-haves #moon_partner  http://bit.ly/kendall-moon-t  pic.twitter.com/UdIpCrkaFf\n",
       "4     Good things come in threes ❤️  http://proactv.us/hLvNMI  #proactiv_ambassador https://twitter.com/Proactiv/status/1153318858327044096 …\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_tweets = copy.deepcopy(kardashians['tweet'])\n",
    "raw_tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atwood\n",
    "# Trump\n",
    "raw_tweets = raw_tweets.append(pd.Series([\"Lennie Goodings's memoir about @ViragoBooks, forthcoming from @OUP, is now called A Bite of the Apple. All an apple should be: crisp, tart but sweet, steeped in mysterious history and tangled symbolism, and not a bad missile when it comes to alleyway combat. Oh, and delicious!\",\n",
    "                             \"The Fake News LameStream Media is doing everything possible the “create” a U.S. recession, even though the numbers & facts are working totally in the opposite direction. They would be willing to hurt many people, but that doesn’t matter to them. Our Economy is sooo strong, sorry!\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tweets = raw_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121391                                                                                                                                                                                                                         Diddy has a Twitterberry and Ryan has a TweetDeck..What are those??\n",
       "121392                                                                                                                                                    hey Ryan Seacrest asked me yesterday if I Twitter and I said \"DO I DO WHAT?!\" So here I am! Come on Kourt Kim Khloe Rob...Where are you?\n",
       "121393                                                                                                                                                                                       This is the first time I am twittering and its really easy! I just invited all of my kids to twitter!\n",
       "121394       Lennie Goodings's memoir about @ViragoBooks, forthcoming from @OUP, is now called A Bite of the Apple. All an apple should be: crisp, tart but sweet, steeped in mysterious history and tangled symbolism, and not a bad missile when it comes to alleyway combat. Oh, and delicious!\n",
       "121395    The Fake News LameStream Media is doing everything possible the “create” a U.S. recession, even though the numbers & facts are working totally in the opposite direction. They would be willing to hurt many people, but that doesn’t matter to them. Our Economy is sooo strong, sorry!\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_tweets.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tweets = raw_tweets.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Cant leave the house without my musthave skincare products    \n",
       "1                                           Im so excited for you  \n",
       "2                                                                  \n",
       "3                                                 my  musthaves    \n",
       "4                                  Good things come in threes      \n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = copy.deepcopy(raw_tweets)\n",
    "\n",
    "tweets = tweets.str.replace('[a-z]*\\d+[a-z]*', '')\n",
    "tweets = tweets.str.replace(r'http\\S+', '')\n",
    "tweets = tweets.str.replace(r'pic.twitter.*', '')\n",
    "tweets = tweets.str.replace('(@[A-Za-z0-9]+)', '')\n",
    "tweets = tweets.str.replace('(#[A-Za-z0-9]+)', '')\n",
    "tweets = tweets.str.replace(r'[^\\w\\s]', '')\n",
    "tweets = tweets.str.replace('\\d+', '')\n",
    "tweets = tweets.str.replace('(\\s)_\\w+', '')\n",
    "\n",
    "tweets.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try topic modeling ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim\n",
    "#from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "# sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "# USE TFIDF and LDA\n",
    "# tweet | username | topic1 | topic2 | ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab tweets (data) and corresponding usernames (targets) seperately, but PRESERVE ROW ORDER\n",
    "#tweets = kardashians['tweet']\n",
    "usernames = kardashians['username']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=10,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a CountVectorizer for parsing/counting words\n",
    "count_vectorizer = TfidfVectorizer(\n",
    "                                   tokenizer=None,\n",
    "                                   ngram_range=(1,3),\n",
    "                                   stop_words=None,\n",
    "                                   min_df=10,\n",
    "                                   #strip_accents='unicode',\n",
    "                                   #sublinear_tf=True\n",
    "    \n",
    ")\n",
    "\n",
    "# count_vectorizer = CountVectorizer(\n",
    "#                                    analyzer='word',\n",
    "#                                    ngram_range=(1, 2),  \n",
    "#                                    stop_words='english',\n",
    "#                                    token_pattern=\"\\\\b[a-z][a-z]+\\\\b\"\n",
    "#                                   )\n",
    "\n",
    "count_vectorizer.fit(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train set and test set\n",
    "\n",
    "# KARDASHIAN BY TIME (RECENT / TOTAL / ETC...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121396, 23462)"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the transposed \n",
    "doc_term = count_vectorizer.transform(tweets) # JUST TRANSFORM TWEETS FROM OTHERS\n",
    "doc_term.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "corpus = matutils.Sparse2Corpus(doc_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23462"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word = dict((v, k) for k, v in count_vectorizer.vocabulary_.items())\n",
    "len(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LDA model (equivalent to \"fit\" in sklearn)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=20, max_iter=6, learning_method='online', learning_offset=50.,random_state=0).fit(doc_term)\n",
    "\n",
    "#lda = models.LdaModel(corpus=corpus, num_topics=10, id2word=id2word, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23462"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lda.components_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__', '__rowland', '_a', '_army', '_b', '_bailon']"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names()[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  1\n",
      "love, love you, you, more, you more, love you more, too, you love, guys, you guys\n",
      "\n",
      "Topic  2\n",
      "muah, am, why thank, why thank you, tonite, baby muah, moment, lover, for this, interview\n",
      "\n",
      "Topic  3\n",
      "miss, dolls, you, miss you, dont, will, can, we, why, it\n",
      "\n",
      "Topic  4\n",
      "lol, is, the, night, so, best, me, omg, this, the best\n",
      "\n",
      "Topic  5\n",
      "to, the, my, cant, for, wait, cant wait, on, with, vote\n",
      "\n",
      "Topic  6\n",
      "yay, gorgeous, my, loves, peaches, bible, on my, bless you, app, my app\n",
      "\n",
      "Topic  7\n",
      "preach, boo, kourtney, kim, great, big, ladies, take miami, give, awwwwww\n",
      "\n",
      "Topic  8\n",
      "proud, hahahaha, proud of, so proud, so proud of, of, sister, kuwtk, of you, sunday\n",
      "\n",
      "Topic  9\n",
      "hahaha, your, stop, you, whats, know, need, loving, face, life\n",
      "\n",
      "Topic  10\n",
      "excited, at, for, the, on, so excited, our, get, my, to\n",
      "\n",
      "Topic  11\n",
      "on, tonight, coast, watch, episode, lamar, the, east, in, tune\n",
      "\n",
      "Topic  12\n",
      "fun, fab, die, much, much fun, agree, so much, quick, so much fun, so fun\n",
      "\n",
      "Topic  13\n",
      "happy, baby, birthday, happy birthday, you baby, xoxo, you, love you, love, love you baby\n",
      "\n",
      "Topic  14\n",
      "thank, thank you, you, so, love, for, thank you baby, thanks, much, so much\n",
      "\n",
      "Topic  15\n",
      "yes, miami, man, cray, silly, everything, boy, cry, kourtney and, please\n",
      "\n",
      "Topic  16\n",
      "morning, god, oh, hey, good, go, nyc, no, god bless, good morning\n",
      "\n",
      "Topic  17\n",
      "the, and, of, to, just, my, photo, awww, in, back\n",
      "\n",
      "Topic  18\n",
      "amen, kardashian, honey, luck, obsessed, vegas, make, good luck, kollection, here\n",
      "\n",
      "Topic  19\n",
      "hi, doll, baby, are, awwww, hi baby, you, my, going, are you\n",
      "\n",
      "Topic  20\n",
      "to, it, do, that, and, we, lol, in, the, is\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 10\n",
    "tfidf_feature_names = count_vectorizer.get_feature_names()\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix+1)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\", topic_names[ix], \"'\")\n",
    "        print(\", \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "display_topics(lda, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01170491, 0.01170491, 0.20618008, ..., 0.08770173, 0.38805933,\n",
       "        0.01170491],\n",
       "       [0.01165672, 0.01165672, 0.01165672, ..., 0.01165672, 0.01165672,\n",
       "        0.01165672],\n",
       "       [0.05      , 0.05      , 0.05      , ..., 0.05      , 0.05      ,\n",
       "        0.05      ],\n",
       "       ...,\n",
       "       [0.00791483, 0.00791483, 0.00791483, ..., 0.00791483, 0.1006578 ,\n",
       "        0.08541678],\n",
       "       [0.0073222 , 0.06316579, 0.0073222 , ..., 0.06661894, 0.0073222 ,\n",
       "        0.19680794],\n",
       "       [0.03213872, 0.0282394 , 0.11600626, ..., 0.00644747, 0.07343502,\n",
       "        0.03214437]])"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the docs from the word space to the topic space (like \"transform\" in sklearn)\n",
    "lda_corpus = lda.transform(doc_term)\n",
    "lda_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the documents' topic vectors in a list so we can take a peak\n",
    "lda_docs = [doc for doc in lda_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01170491, 0.01170491, 0.20618008, 0.01170491, 0.01170491,\n",
       "       0.01170491, 0.01170491, 0.01170491, 0.01170491, 0.01170491,\n",
       "       0.01170491, 0.01170491, 0.13078023, 0.01170491, 0.01170491,\n",
       "       0.01170491, 0.01170491, 0.08770173, 0.38805933, 0.01170491])"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the document vectors in the topic space for the first 5 documents\n",
    "lda_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    " topic_names = ['topic01', 'topic02', 'topic03', 'topic04', 'topic05', 'topic06', 'topic07', 'topic08',\n",
    "                'topic09', 'topic10', 'topic11' ,'topic12' ,'topic13' ,'topic14' ,'topic15' ,'topic16',\n",
    "                'topic17', 'topic18', 'topic19', 'topic20']\n",
    "\n",
    "# topic_names = ['topic00', 'topic01', 'topic02', 'topic03', 'topic04', 'topic05', 'topic06', 'topic07',\n",
    "#                'topic08', 'topic09']\n",
    "\n",
    "# topic_names = ['topic00', 'topic01', 'topic02', 'topic03', 'topic04', 'topic05', 'topic06', 'topic07',\n",
    "#                'topic08', 'topic09', 'topic10', 'topic11', 'topic12', 'topic13', 'topic14']\n",
    "\n",
    "topics_df = pd.DataFrame(lda_docs, columns=topic_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121396, 20)"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "atwood_trump = topics_df.iloc[-2:,:]\n",
    "topics_df = topics_df.iloc[:-2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121394, 20)"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kardashians.shape\n",
    "topics_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "kardashians.reset_index(inplace=True, drop=True)\n",
    "topics_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "      <th>topic01</th>\n",
       "      <th>topic02</th>\n",
       "      <th>topic03</th>\n",
       "      <th>topic04</th>\n",
       "      <th>topic05</th>\n",
       "      <th>topic06</th>\n",
       "      <th>topic07</th>\n",
       "      <th>topic08</th>\n",
       "      <th>topic09</th>\n",
       "      <th>topic10</th>\n",
       "      <th>topic11</th>\n",
       "      <th>topic12</th>\n",
       "      <th>topic13</th>\n",
       "      <th>topic14</th>\n",
       "      <th>topic15</th>\n",
       "      <th>topic16</th>\n",
       "      <th>topic17</th>\n",
       "      <th>topic18</th>\n",
       "      <th>topic19</th>\n",
       "      <th>topic20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-08-08</td>\n",
       "      <td>10:06:42</td>\n",
       "      <td>kendalljenner</td>\n",
       "      <td>Can’t leave the house without my must-have skincare products! #proactiv_ambassador  http://proactv.us/NgPyPZ  pic.twitter.com/si6kAzr6qg</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-08-04</td>\n",
       "      <td>12:48:09</td>\n",
       "      <td>kendalljenner</td>\n",
       "      <td>I’m so excited for you! ❤️#proactiv_ambassador https://twitter.com/mhabeck89/status/1143325557574295552 …</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      time       username                                                                                                                                     tweet  topic01  topic02  topic03  topic04  topic05  topic06  topic07  topic08  topic09  topic10  topic11  topic12  topic13  topic14  topic15  topic16  topic17  topic18  topic19  topic20\n",
       "0  2019-08-08  10:06:42  kendalljenner  Can’t leave the house without my must-have skincare products! #proactiv_ambassador  http://proactv.us/NgPyPZ  pic.twitter.com/si6kAzr6qg    0.012    0.012    0.206    0.012    0.012    0.012    0.012    0.012    0.012    0.012    0.012    0.012    0.131    0.012    0.012    0.012    0.012    0.088    0.388    0.012\n",
       "1  2019-08-04  12:48:09  kendalljenner                                 I’m so excited for you! ❤️#proactiv_ambassador https://twitter.com/mhabeck89/status/1143325557574295552 …    0.012    0.012    0.012    0.012    0.012    0.012    0.012    0.012    0.012    0.779    0.012    0.012    0.012    0.012    0.012    0.012    0.012    0.012    0.012    0.012"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_kardash = pd.concat([kardashians, topics_df], axis=1)\n",
    "lda_kardash.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121394, 24)"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_kardash.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic01</th>\n",
       "      <th>topic02</th>\n",
       "      <th>topic03</th>\n",
       "      <th>topic04</th>\n",
       "      <th>topic05</th>\n",
       "      <th>topic06</th>\n",
       "      <th>topic07</th>\n",
       "      <th>topic08</th>\n",
       "      <th>topic09</th>\n",
       "      <th>topic10</th>\n",
       "      <th>topic11</th>\n",
       "      <th>topic12</th>\n",
       "      <th>topic13</th>\n",
       "      <th>topic14</th>\n",
       "      <th>topic15</th>\n",
       "      <th>topic16</th>\n",
       "      <th>topic17</th>\n",
       "      <th>topic18</th>\n",
       "      <th>topic19</th>\n",
       "      <th>topic20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>username</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kendalljenner</th>\n",
       "      <td>0.048</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>khloekardashian</th>\n",
       "      <td>0.063</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kimkardashian</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kourtneykardash</th>\n",
       "      <td>0.042</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>krisjenner</th>\n",
       "      <td>0.044</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kyliejenner</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 topic01  topic02  topic03  topic04  topic05  topic06  topic07  topic08  topic09  topic10  topic11  topic12  topic13  topic14  topic15  topic16  topic17  topic18  topic19  topic20\n",
       "username                                                                                                                                                                                           \n",
       "kendalljenner      0.048    0.024    0.057    0.086    0.072    0.032    0.027    0.030    0.046    0.074    0.048    0.032    0.040    0.046    0.028    0.039    0.079    0.041    0.065    0.087\n",
       "khloekardashian    0.063    0.028    0.061    0.072    0.058    0.033    0.026    0.028    0.041    0.059    0.052    0.026    0.047    0.073    0.026    0.034    0.069    0.037    0.077    0.089\n",
       "kimkardashian      0.050    0.025    0.052    0.073    0.081    0.030    0.028    0.029    0.040    0.082    0.059    0.029    0.039    0.049    0.026    0.036    0.076    0.048    0.062    0.085\n",
       "kourtneykardash    0.042    0.025    0.052    0.067    0.078    0.031    0.030    0.030    0.042    0.084    0.077    0.028    0.038    0.036    0.028    0.037    0.076    0.047    0.057    0.095\n",
       "krisjenner         0.044    0.021    0.046    0.065    0.079    0.024    0.026    0.031    0.032    0.084    0.125    0.025    0.043    0.057    0.022    0.032    0.071    0.043    0.054    0.076\n",
       "kyliejenner        0.050    0.027    0.054    0.075    0.072    0.035    0.029    0.029    0.043    0.086    0.054    0.030    0.038    0.046    0.031    0.034    0.075    0.050    0.057    0.088"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_kardash.groupby('username').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Train-test split\n",
    "lda_kardash.username[lda_kardash.username == 'krisjenner'] = 1\n",
    "lda_kardash.username[lda_kardash.username == 'kyliejenner'] = 2\n",
    "lda_kardash.username[lda_kardash.username == 'kendalljenner'] = 3\n",
    "lda_kardash.username[lda_kardash.username == 'kimkardashian'] = 4\n",
    "lda_kardash.username[lda_kardash.username == 'khloekardashian'] = 5\n",
    "lda_kardash.username[lda_kardash.username == 'kourtneykardash'] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lda_kardash.username\n",
    "X = lda_kardash.iloc[:, 4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121394, 24)"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_kardash.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "y_train = y_train.astype('int')\n",
    "y_test  = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(solver='lbfgs', multi_class='multinomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype('int')\n",
    "y_test  = y_test.astype('int')\n",
    "lr_fit = lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = lr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, ..., 5, 5, 5])"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4530664360146629"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "     krisjenner       0.00      0.00      0.00      1822\n",
      "    kyliejenner       0.00      0.00      0.00      2053\n",
      "  kendalljenner       0.00      0.00      0.00      1594\n",
      "  kimkardashian       0.50      0.00      0.00      5399\n",
      "khloekardashian       0.45      1.00      0.62     10999\n",
      "kourtneykardash       0.00      0.00      0.00      2412\n",
      "\n",
      "      micro avg       0.45      0.45      0.45     24279\n",
      "      macro avg       0.16      0.17      0.10     24279\n",
      "   weighted avg       0.32      0.45      0.28     24279\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "target_names = ['krisjenner', 'kyliejenner', 'kendalljenner', 'kimkardashian',\n",
    "                'khloekardashian', 'kourtneykardash']\n",
    "\n",
    "print(classification_report(y_test, y_predict, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=None, max_features='sqrt',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=500, n_jobs=None, oob_score=False,\n",
       "            random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model with 100 trees\n",
    "rfm = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    #criterion='gini',\n",
    "    #max_depth=None,\n",
    "    #min_samples_split=2,\n",
    "    #min_samples_leaf=1,\n",
    "    #min_weight_fraction_leaf=0,\n",
    "    max_features='sqrt',\n",
    "    #max_leaf_nodes=None,\n",
    "    #min_impurity_decrease=0,\n",
    "    #min_impurity_split=1e-7,\n",
    "    #bootstrap=True,\n",
    "    #oob_score=False,\n",
    "    #n_jobs=None,\n",
    "    #random_state=None,\n",
    "    #verbose=0,\n",
    "    #warm_start=False,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "rfm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic01</th>\n",
       "      <th>topic02</th>\n",
       "      <th>topic03</th>\n",
       "      <th>topic04</th>\n",
       "      <th>topic05</th>\n",
       "      <th>topic06</th>\n",
       "      <th>topic07</th>\n",
       "      <th>topic08</th>\n",
       "      <th>topic09</th>\n",
       "      <th>topic10</th>\n",
       "      <th>topic11</th>\n",
       "      <th>topic12</th>\n",
       "      <th>topic13</th>\n",
       "      <th>topic14</th>\n",
       "      <th>topic15</th>\n",
       "      <th>topic16</th>\n",
       "      <th>topic17</th>\n",
       "      <th>topic18</th>\n",
       "      <th>topic19</th>\n",
       "      <th>topic20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48073</th>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80962</th>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11470</th>\n",
       "      <td>0.084</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87672</th>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20033</th>\n",
       "      <td>0.139</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116967</th>\n",
       "      <td>0.242</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57508</th>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117761</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83670</th>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99763</th>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13753</th>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7399</th>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102575</th>\n",
       "      <td>0.728</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96885</th>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24279 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        topic01  topic02  topic03  topic04  topic05  topic06  topic07  topic08  topic09  topic10  topic11  topic12  topic13  topic14  topic15  topic16  topic17  topic18  topic19  topic20\n",
       "48073     0.012    0.012    0.012    0.057    0.244    0.334    0.012    0.012    0.012    0.012    0.178    0.012    0.012    0.012    0.012    0.012    0.012    0.012    0.012    0.012\n",
       "80962     0.015    0.015    0.015    0.015    0.015    0.322    0.015    0.015    0.015    0.155    0.015    0.015    0.267    0.015    0.015    0.015    0.015    0.015    0.015    0.015\n",
       "11470     0.084    0.264    0.010    0.010    0.010    0.010    0.082    0.010    0.070    0.233    0.010    0.090    0.010    0.010    0.010    0.044    0.010    0.010    0.010    0.010\n",
       "87672     0.013    0.013    0.013    0.013    0.013    0.013    0.013    0.013    0.013    0.013    0.013    0.013    0.244    0.526    0.013    0.013    0.013    0.013    0.013    0.013\n",
       "20033     0.139    0.012    0.202    0.012    0.012    0.012    0.012    0.012    0.012    0.411    0.012    0.012    0.012    0.012    0.012    0.012    0.012    0.012    0.012    0.062\n",
       "116967    0.242    0.008    0.008    0.008    0.399    0.008    0.008    0.008    0.008    0.170    0.008    0.008    0.008    0.008    0.008    0.008    0.057    0.008    0.008    0.008\n",
       "57508     0.017    0.017    0.017    0.017    0.017    0.017    0.215    0.017    0.017    0.017    0.017    0.017    0.017    0.017    0.017    0.017    0.017    0.471    0.017    0.017\n",
       "...         ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...\n",
       "117761    0.008    0.008    0.037    0.078    0.050    0.008    0.008    0.008    0.008    0.008    0.008    0.008    0.367    0.074    0.040    0.008    0.187    0.076    0.008    0.008\n",
       "83670     0.014    0.014    0.341    0.014    0.014    0.152    0.014    0.014    0.014    0.275    0.014    0.014    0.014    0.014    0.014    0.014    0.014    0.014    0.014    0.014\n",
       "99763     0.018    0.018    0.018    0.018    0.018    0.018    0.018    0.018    0.018    0.018    0.018    0.018    0.018    0.018    0.018    0.018    0.018    0.018    0.649    0.018\n",
       "13753     0.013    0.013    0.013    0.474    0.155    0.013    0.013    0.013    0.013    0.013    0.013    0.013    0.013    0.013    0.013    0.013    0.013    0.013    0.145    0.013\n",
       "7399      0.011    0.011    0.011    0.011    0.011    0.011    0.011    0.011    0.068    0.234    0.011    0.124    0.112    0.011    0.011    0.011    0.011    0.011    0.011    0.298\n",
       "102575    0.728    0.014    0.014    0.014    0.014    0.014    0.014    0.014    0.014    0.014    0.014    0.014    0.014    0.014    0.014    0.014    0.014    0.014    0.014    0.014\n",
       "96885     0.012    0.012    0.012    0.553    0.012    0.012    0.012    0.012    0.012    0.012    0.012    0.012    0.012    0.012    0.012    0.012    0.162    0.082    0.012    0.012\n",
       "\n",
       "[24279 rows x 20 columns]"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'concat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-573-98bc04b7b7f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matwood_trump\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5065\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5066\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5067\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'concat'"
     ]
    }
   ],
   "source": [
    "X_test.concat(atwood_trump)\n",
    "X_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "train_rfm_predictions = rfm.predict(X_train)\n",
    "train_rfm_probs = rfm.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# test set\n",
    "y_predict = rfm.predict(X_test)\n",
    "rfm_probs = rfm.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5])"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "at_tr_predict = rfm.predict(atwood_trump)\n",
    "at_tr_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "     krisjenner       0.27      0.11      0.15      1822\n",
      "    kyliejenner       0.21      0.21      0.21      2053\n",
      "  kendalljenner       0.21      0.11      0.15      1594\n",
      "  kimkardashian       0.35      0.14      0.20      5399\n",
      "khloekardashian       0.50      0.81      0.62     10999\n",
      "kourtneykardash       0.21      0.06      0.10      2412\n",
      "\n",
      "      micro avg       0.44      0.44      0.44     24279\n",
      "      macro avg       0.29      0.24      0.24     24279\n",
      "   weighted avg       0.38      0.44      0.37     24279\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['krisjenner', 'kyliejenner', 'kendalljenner', 'kimkardashian',\n",
    "                'khloekardashian', 'kourtneykardash']\n",
    "\n",
    "print(classification_report(y_test, y_predict, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import numpy as np\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.plotting import save\n",
    "from bokeh.models import HoverTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.184383  , 0.02928128, 0.02928128, ..., 0.13299893, 0.33593639,\n",
       "        0.17099202],\n",
       "       [0.03716068, 0.03716049, 0.03716177, ..., 0.03716287, 0.03716095,\n",
       "        0.03716084],\n",
       "       [0.1       , 0.1       , 0.1       , ..., 0.1       , 0.1       ,\n",
       "        0.1       ],\n",
       "       ...,\n",
       "       [0.31562109, 0.04146975, 0.04146975, ..., 0.3526209 , 0.04146975,\n",
       "        0.04146975],\n",
       "       [0.09393234, 0.02152065, 0.02152088, ..., 0.26629844, 0.1500689 ,\n",
       "        0.02152712],\n",
       "       [0.02686998, 0.14090944, 0.02686998, ..., 0.25439482, 0.18826886,\n",
       "        0.02688295]])"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_vectors = np.array(topics_df)\n",
    "topic_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 121394 samples in 0.626s...\n",
      "[t-SNE] Computed neighbors for 121394 samples in 533.701s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 11000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 12000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 13000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 14000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 15000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 16000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 17000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 18000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 19000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 20000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 21000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 22000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 23000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 24000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 25000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 26000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 27000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 28000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 29000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 30000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 31000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 32000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 33000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 34000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 35000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 36000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 37000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 38000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 39000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 40000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 41000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 42000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 43000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 44000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 45000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 46000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 47000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 48000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 49000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 50000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 51000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 52000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 53000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 54000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 55000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 56000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 57000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 58000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 59000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 60000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 61000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 62000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 63000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 64000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 65000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 66000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 67000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 68000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 69000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 70000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 71000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 72000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 73000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 74000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 75000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 76000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 77000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 78000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 79000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 80000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 81000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 82000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 83000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 84000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 85000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 86000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 87000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 88000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 89000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 90000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 91000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 92000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 93000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 94000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 95000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 96000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 97000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 98000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 99000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 100000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 101000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 102000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 103000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 104000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 105000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 106000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 107000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 108000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 109000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 110000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 111000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 112000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 113000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 114000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 115000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 116000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 117000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 118000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 119000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 120000 / 121394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computed conditional probabilities for sample 121000 / 121394\n",
      "[t-SNE] Computed conditional probabilities for sample 121394 / 121394\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 109.003845\n",
      "[t-SNE] KL divergence after 1000 iterations: 3.337443\n"
     ]
    }
   ],
   "source": [
    "tsne_lda = tsne_model.fit_transform(topic_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 5\n",
    "\n",
    "colormap = np.array([\n",
    "    \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\",\n",
    "    \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#c5b0d5\",\n",
    "    \"#8c564b\", \"#c49c94\", \"#e377c2\", \"#f7b6d2\", \"#7f7f7f\",\n",
    "    \"#c7c7c7\", \"#bcbd22\", \"#dbdb8d\", \"#17becf\", \"#9edae5\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xrange' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-324-a3d834f3f71a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m_lda_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0m_lda_keys\u001b[0m \u001b[0;34m+=\u001b[0m  \u001b[0m_topics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xrange' is not defined"
     ]
    }
   ],
   "source": [
    "_lda_keys = []\n",
    "for i in xrange(topic_vectors.shape[0]):\n",
    "  _lda_keys +=  _topics[i].argmax(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "nn.fit(topic_vectors)\n",
    "\n",
    "word_indices = set()\n",
    "\n",
    "key_words = ['sweet', 'proud', 'baby', 'khloe', 'thanks', 'yes', 'today', 'miss', 'good',\n",
    "             'know', 'thank', 'fun', 'dolls', 'amazing', 'love', 'hi', 'muah', 'true', 'wow', 'yay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "#key_words = [\"monster\", \"margaret\", \"elizabeth\", \"victor\", \"robert\", \"frankenstein\", \"henry\", \"justine\", \"beaufort\", \"caroline\", \"william\"]\n",
    "for key in key_words:\n",
    "    word_index = count_vectorizer.vocabulary_[key]\n",
    "    dist, index = nn.kneighbors(topic_vectors[[word_index], :], n_neighbors=10)\n",
    "    word_indices.add(word_index)\n",
    "    word_indices.update(list(index.flat))\n",
    "    \n",
    "word_indices = list(word_indices)\n",
    "\n",
    "\n",
    "target_vecs = topic_vectors[word_indices,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'seaborn' has no attribute 'annotate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-314-1e63053fef6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"red\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"round\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"k\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzorder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"round\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"k\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzorder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'seaborn' has no attribute 'annotate'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#figsize(10,10)\n",
    "\n",
    "vecs_emb = manifold.TSNE( metric=\"cosine\", n_iter=3000).fit_transform(target_vecs)\n",
    "\n",
    "sns.scatterplot(x=vecs_emb[:,0], y=vecs_emb[:,1],marker='.' )\n",
    "for i, row in enumerate(vecs_emb):\n",
    "    word_i = word_indices[i]\n",
    "    word = count_vectorizer.get_feature_names()[word_i]\n",
    "    if word in key_words:\n",
    "        annotate(word, row, color=\"red\", bbox=dict(boxstyle=\"round\", fc=\"w\", ec=\"k\", alpha=.5), zorder=10)\n",
    "    else:\n",
    "        annotate(word, row, bbox=dict(boxstyle=\"round\", fc=\"w\", ec=\"k\", alpha=.5), zorder=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
